{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CatsDogs.simple.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmschulman/mldata/blob/master/CatsDogs_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cubEmxThExcZ",
        "colab_type": "text"
      },
      "source": [
        "### First let us import all the required keras packages using which we are going to build our CNN, make sure that every package is installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rToMgInPFFyF",
        "colab_type": "code",
        "outputId": "6526c0fd-19eb-4d09-a1a7-4e41f8722d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nlo28XJ6FO0a",
        "colab_type": "text"
      },
      "source": [
        "In line 1, we’ve imported Sequential from keras.models, to initialise our neural network model as a sequential network. There are two basic ways of initialising a neural network, either by a sequence of layers or as a graph.\n",
        "\n",
        "In line 2, we’ve imported Conv2D from keras.layers, this is to perform the convolution operation i.e the first step of a CNN, on the training images. Since we are working on images here, which a basically 2 Dimensional arrays, we’re using Convolution 2-D, you may have to use Convolution 3-D while dealing with videos, where the third dimension will be time.\n",
        "\n",
        "In line 3, we’ve imported MaxPooling2D from keras.layers, which is used for pooling operation, that is the step — 2 in the process of building a cnn. For building this particular neural network, we are using a Maxpooling function, there exist different types of pooling operations like Min Pooling, Mean Pooling, etc. Here in MaxPooling we need the maximum value pixel from the respective region of interest.\n",
        "\n",
        "In line 4, we’ve imported Flatten from keras.layers, which is used for Flattening. Flattening is the process of converting all the resultant 2 dimensional arrays into a single long continuous linear vector.\n",
        "\n",
        "And finally in line 5, we’ve imported Dense from keras.layers, which is used to perform the full connection of the neural network, which is the step 4 in the process of building a CNN.\n",
        "\n",
        "Now, we will create an object of the sequential class below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhQQ7-1rFePp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piu_J2QlFiE4",
        "colab_type": "text"
      },
      "source": [
        "Let us now code the Convolution step, you will be surprised to see how easy it is to actually implement these complex operations in a single line of code in python, thanks to Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQJXVTAaFlGz",
        "colab_type": "code",
        "outputId": "eefcd0dc-ac4b-4ebb-cf99-dc640937dbb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFRqIAx2Fr52",
        "colab_type": "text"
      },
      "source": [
        "Let’s break down the above code function by function. We took the object which already has an idea of how our neural network is going to be(Sequential), then we added a convolution layer by using the “Conv2D” function. The Conv2D function is taking 4 arguments, the first is the number of filters i.e 32 here, the second argument is the shape each filter is going to be i.e 3x3 here, the third is the input shape and the type of image(RGB or Black and White)of each image i.e the input image our CNN is going to be taking is of a 64x64 resolution and “3” stands for RGB, which is a colour img, the fourth argument is the activation function we want to use, here ‘relu’ stands for a rectifier function.\n",
        "\n",
        "---\n",
        "\n",
        "Now, we need to perform pooling operation on the resultant feature maps we get after the convolution operation is done on an image. The primary aim of a pooling operation is to reduce the size of the images as much as possible. In order to understand what happens in these steps in more detail you need to read few external resources. But the key thing to understand here is that we are trying to reduce the total number of nodes for the upcoming layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCM1hpqSFw9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blwEkHp3F1G7",
        "colab_type": "text"
      },
      "source": [
        "We start by taking our classifier object and add the pooling layer. We take a 2x2 matrix we’ll have minimum pixel loss and get a precise region where the feature are located. Again, to understand the actual math behind Pooling, i suggest you to go learn from an external source, this tutorial concentrates more on the implementation part. We just reduced the complexity of the model without reducing it’s performance.\n",
        "\n",
        "---\n",
        "\n",
        "It’s time for us to now convert all the pooled images into a continuous vector through Flattening. Flattening is a very important step to understand. What we are basically doing here is taking the 2-D array, i.e pooled image pixels and converting them to a one dimensional single vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdosvwhLF55J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.add(Flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xELhbXI-F-R6",
        "colab_type": "text"
      },
      "source": [
        "In this step we need to create a fully connected layer, and to this layer we are going to connect the set of nodes we got after the flattening step, these nodes will act as an input layer to these fully-connected layers. As this layer will be present between the input layer and output layer, we can refer to it a hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnXHr2RrGAZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.add(Dense(units = 128, activation = 'relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL6xEYyaGDHR",
        "colab_type": "text"
      },
      "source": [
        "As you can see, Dense is the function to add a fully connected layer, ‘units’ is where we define the number of nodes that should be present in this hidden layer, these units value will be always between the number of input nodes and the output nodes but the art of choosing the most optimal number of nodes can be achieved only through experimental tries. Though it’s a common practice to use a power of 2. And the activation function will be a rectifier function.\n",
        "\n",
        "---\n",
        "\n",
        "Now it’s time to initialise our output layer, which should contain only one node, as it is binary classification. This single node will give us a binary output of either a Cat or Dog."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH-Ug7feGHoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpsLZVunGLKh",
        "colab_type": "text"
      },
      "source": [
        "Observe that the final layer contains only one node, and we will be using a sigmoid activation function for the final layer.  Why just one node?  Because it is classifying dog vs cat.\n",
        "\n",
        "---\n",
        "\n",
        "Now that we have completed building our CNN model, it’s time to compile it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xady9AX4GYNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRCEDI1MGbLu",
        "colab_type": "text"
      },
      "source": [
        "*   Optimizer parameter is to choose the stochastic gradient descent algorithm.\n",
        "*   Loss parameter is to choose the loss function.\n",
        "*   Finally, the metrics parameter is to choose the performance metric.\n",
        "\n",
        "---\n",
        "\n",
        "It’s time to fit our CNN to the image dataset that you’ve downloaded.But before we do that, we are going to pre-process the images to prevent over-fitting. Overfitting is when you get a great training accuracy and very poor test accuracy due to overfitting of nodes from one layer to another.\n",
        "So before we fit our images to the neural network, we need to perform some image augmentations on them, which is basically synthesising the training data. We are going to do this using keras.preprocessing library for doing the synthesising part as well as to prepare the training set as well as the test test set of images that are present in a properly structured directories, where the directory’s name is take as the label of all the images present in it. For example : All the images inside the ‘cats’ named folder will be considered as cats by keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-zUCBeUtCLq",
        "colab_type": "text"
      },
      "source": [
        "Load the training and test data from github.  If you've already set up the dataset directory, don't do this again.  You use **Runtime > Run before** and **Runtime > Run after** to control whether to run this step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrgC9ZQorHH8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8ad7717b-3c80-427a-e98c-319af0578192"
      },
      "source": [
        "!git clone https://github.com/rmschulman/mldata.git\n",
        "!mv mldata/dataset .\n",
        "!rm -r mldata"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mldata'...\n",
            "remote: Enumerating objects: 1976, done.\u001b[K\n",
            "remote: Counting objects: 100% (1976/1976), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1971/1971), done.\u001b[K\n",
            "remote: Total 1976 (delta 4), reused 1973 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1976/1976), 18.81 MiB | 23.15 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mJjzdJwGvG4",
        "colab_type": "code",
        "outputId": "912c5d0b-9cca-45d5-cd2a-7e83e50cb685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "   shear_range = 0.2,\n",
        "   zoom_range = 0.2,\n",
        "   horizontal_flip = True)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
        "   target_size = (64, 64),\n",
        "   batch_size = 32,\n",
        "   class_mode = 'binary')\n",
        "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
        "   target_size = (64, 64),\n",
        "   batch_size = 32,\n",
        "   class_mode = 'binary')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1589 images belonging to 2 classes.\n",
            "Found 372 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UXeCmg6G2Yk",
        "colab_type": "text"
      },
      "source": [
        "Now lets fit the data to our model.  In other words, train the model using the training data.\n",
        "\n",
        "‘steps_per_epoch’ holds the number of training images, i.e the number of images the training_set folder contains.\n",
        "\n",
        "A single epoch is a single step in training a neural network; in other words when a neural network is trained on every training samples only in one pass we say that one epoch is finished. So training process should consist more than one epochs.In this case we have defined 25 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPwM54IvG48m",
        "colab_type": "code",
        "outputId": "fa9142d2-1a36-4ca2-83a1-20d3ff3b156b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "classifier.fit_generator(training_set,\n",
        "steps_per_epoch = 500, # was 8000\n",
        "epochs = 5, # was 25\n",
        "validation_data = test_set,\n",
        "validation_steps = 200) # was 2000"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "500/500 [==============================] - 138s 275ms/step - loss: 0.5006 - acc: 0.7517 - val_loss: 0.6650 - val_acc: 0.7068\n",
            "Epoch 2/5\n",
            "500/500 [==============================] - 136s 271ms/step - loss: 0.3651 - acc: 0.8351 - val_loss: 0.6958 - val_acc: 0.7084\n",
            "Epoch 3/5\n",
            "500/500 [==============================] - 135s 271ms/step - loss: 0.2563 - acc: 0.8940 - val_loss: 1.1152 - val_acc: 0.6603\n",
            "Epoch 4/5\n",
            "500/500 [==============================] - 134s 267ms/step - loss: 0.1860 - acc: 0.9293 - val_loss: 0.9222 - val_acc: 0.7313\n",
            "Epoch 5/5\n",
            "500/500 [==============================] - 134s 267ms/step - loss: 0.1243 - acc: 0.9547 - val_loss: 1.0429 - val_acc: 0.7389\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efec665c668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwj-sBL3HSGO",
        "colab_type": "text"
      },
      "source": [
        "# Make predictions using our trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_1QpKVutjzO",
        "colab_type": "text"
      },
      "source": [
        "Find a test image and try it out!\n",
        "\n",
        "Upload it into the directory **single_prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoeIaFV_t8QO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p single_prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMo5dPTzHVjM",
        "colab_type": "code",
        "outputId": "57c1f3ab-384d-4178-944f-5f06dfaf13b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('single_prediction/cat.2.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = classifier.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'\n",
        "print ('result = ', prediction)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d7efd8175c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'single_prediction/cat.2.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    102\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    103\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'single_prediction/cat.2.jpg'"
          ]
        }
      ]
    }
  ]
}